{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEUjNrhz3p-y"
   },
   "source": [
    "# Word Embeddings\n",
    "    More details in the official documentation: https://radimrehurek.com/gensim/auto_examples/index.html#documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "TVVgZCg0RUox"
   },
   "outputs": [],
   "source": [
    "# !pip install gensim --upgrade\n",
    "# !pip install numpy --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.1.2\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PJ6gpAJCop9D"
   },
   "outputs": [],
   "source": [
    "from gensim import downloader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6G8nu1W3wtX"
   },
   "source": [
    "## Loading The Pretrained Weights\n",
    "Supported options are at https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "1ieBE5Zo7-iv"
   },
   "outputs": [],
   "source": [
    "WORD_2_VEC_PATH = 'word2vec-google-news-300'\n",
    "GLOVE_PATH = 'glove-twitter-200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TqRr4ctUpAK4"
   },
   "outputs": [],
   "source": [
    "glove = downloader.load(GLOVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FOkwYA_8iRU"
   },
   "source": [
    "## Using The Pre-Trained Vecotors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1631532904080,
     "user": {
      "displayName": "תומר וולק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDAXM5v2by4xMwnYYySIn2IrNiidPslt6u4Eez1g=s64",
      "userId": "10359306600558254546"
     },
     "user_tz": -180
    },
    "id": "ukua0IZBpm-Z",
    "outputId": "fe045773-32f5-429d-8efa-90d61090b8e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "technion not an existing word in the model\n",
      "(6, 200)\n"
     ]
    }
   ],
   "source": [
    "sen = \"i am a student at the technion\"\n",
    "representation = []\n",
    "for word in sen.split():\n",
    "    if word not in glove.key_to_index:\n",
    "        print(f\"{word} not an existing word in the model\")\n",
    "        continue\n",
    "    vec = glove[word]\n",
    "    representation.append(vec)\n",
    "representation = np.asarray(representation)\n",
    "print(representation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDecQRKIosCG"
   },
   "source": [
    "# Training A Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "RuDtWHaUosCG",
    "outputId": "f9570197-2965-4432-94d7-f1a844807601"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['chapter', 'i', 'down', 'the', 'rabbithole'],\n",
       " ['alice',\n",
       "  'was',\n",
       "  'beginning',\n",
       "  'to',\n",
       "  'get',\n",
       "  'very',\n",
       "  'tired',\n",
       "  'of',\n",
       "  'sitting',\n",
       "  'by',\n",
       "  'her',\n",
       "  'sister',\n",
       "  'on',\n",
       "  'the']]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT_PATH = 'Alice_book'\n",
    "with open(TEXT_PATH, 'r', encoding='utf-8') as f:\n",
    "    sentences = f.readlines()\n",
    "sentences = [sen.strip().lower() for sen in sentences]\n",
    "sentences = [sen.split() for sen in sentences if sen]\n",
    "sentences = [[re.sub(r'\\W+', '', w) for w in sen] for sen in sentences]\n",
    "sentences[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "0DUA2MososCH"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=sentences, vector_size=10, window=2, min_count=1, workers=4, epochs=100, seed=42)\n",
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "PzS3SHuqosCH",
    "outputId": "6a265bf1-83eb-40ad-b8d3-574e46b4da92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('so', 0.8331175446510315),\n",
       " ('very', 0.8064254522323608),\n",
       " ('late', 0.789730966091156),\n",
       " ('mouse', 0.7821938395500183),\n",
       " ('yet', 0.7783822417259216)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = model.wv.most_similar('alice', topn=5)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('meanwhile', 0.9836915135383606),\n",
       " ('leaders', 0.9790331125259399),\n",
       " ('expected', 0.8927791714668274),\n",
       " ('nothing', 0.8861377835273743),\n",
       " ('hasnt', 0.8848412036895752)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = model.wv.most_similar('annoyed', topn=5)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pissed', 0.8485370874404907),\n",
       " ('irritated', 0.8377761840820312),\n",
       " ('frustrated', 0.7810536026954651),\n",
       " ('annoying', 0.757415771484375),\n",
       " ('upset', 0.7419010996818542)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims = glove.most_similar('annoyed', topn=5)\n",
    "sims"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAuvvflqosCH"
   },
   "source": [
    "# Some Nice Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1631532969645,
     "user": {
      "displayName": "תומר וולק",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDAXM5v2by4xMwnYYySIn2IrNiidPslt6u4Eez1g=s64",
      "userId": "10359306600558254546"
     },
     "user_tz": -180
    },
    "id": "hUUNGQv28e0E",
    "outputId": "491e02d3-6195-4fee-93c1-d6abccb746c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('programs', 0.6853476762771606),\n",
       " ('seminar', 0.6410128474235535),\n",
       " ('training', 0.6214897036552429),\n",
       " ('workshop', 0.591772735118866),\n",
       " ('system', 0.5909943580627441)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar('program', topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wlcfk5NdDxLD",
    "outputId": "7fb94dde-79ea-4716-8380-f787e49617ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6820898056030273),\n",
       " ('prince', 0.5875527262687683),\n",
       " ('princess', 0.5620488524436951),\n",
       " ('royal', 0.5522865653038025),\n",
       " ('mother', 0.5362966656684875)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "yIzy_hAUosCI",
    "outputId": "bf1852d8-50c0-4b03-8542-d7c07b2918cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('france', 0.7369073033332825),\n",
       " ('spain', 0.6768407225608826),\n",
       " ('portugal', 0.6567486524581909),\n",
       " ('italy', 0.6421884298324585),\n",
       " ('denmark', 0.6146384477615356)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(positive=['paris','germany'], negative=['berlin'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "2TP-_X7-osCI",
    "outputId": "d98ce3f0-b33e-4e24-a199-c2071e54ab86"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('walked', 0.5864155888557434),\n",
       " ('drove', 0.5215498805046082),\n",
       " ('ran', 0.5134605169296265),\n",
       " ('sprinted', 0.4759795069694519),\n",
       " ('stood', 0.47308677434921265)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(positive=['walking','swam'], negative=['swimming'], topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-I16V9xosCI"
   },
   "source": [
    "## Sloving NLP Problems - Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentDataSet(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, vector_type, tokenizer=None):\n",
    "        self.file_path = file_path\n",
    "        data = pd.read_csv(self.file_path)\n",
    "        data['label'] = data['label'].replace({'Positive': 1, 'Negative': 0})\n",
    "        self.sentences = data['reviewText'].tolist()\n",
    "        self.labels = data['label'].tolist()\n",
    "        self.tags_to_idx = {tag: idx for idx, tag in enumerate(sorted(list(set(self.labels))))}\n",
    "        self.idx_to_tag = {idx: tag for tag, idx in self.tags_to_idx.items()}\n",
    "        self.vector_type = vector_type\n",
    "        if vector_type == 'tf-idf':\n",
    "            if tokenizer is None:\n",
    "                self.tokenizer = TfidfVectorizer(lowercase=True, stop_words=None)\n",
    "                self.tokenized_sen = self.tokenizer.fit_transform(self.sentences)\n",
    "            else:\n",
    "                self.tokenizer = tokenizer\n",
    "                self.tokenized_sen = self.tokenizer.transform(self.sentences)\n",
    "            self.vector_dim = len(self.tokenizer.vocabulary_)\n",
    "        else:\n",
    "            if vector_type == 'w2v':\n",
    "                model = downloader.load(WORD_2_VEC_PATH)\n",
    "            elif vector_type == 'glove':\n",
    "                model = downloader.load(GLOVE_PATH)\n",
    "            else:\n",
    "                raise KeyError(f\"{vector_type} is not a supported vector type\")\n",
    "            representation, labels = [], []\n",
    "            for sen, cur_labels in zip(self.sentences, self.labels):\n",
    "                cur_rep = []\n",
    "                for word in sen.split():\n",
    "                    word = re.sub(r'\\W+', '', word.lower())\n",
    "                    if word not in model.key_to_index:\n",
    "                        continue\n",
    "                    vec = model[word]\n",
    "                    cur_rep.append(vec)\n",
    "                if len(cur_rep) == 0:\n",
    "                    print(f'Sentence {sen} cannot be represented!')\n",
    "                    continue\n",
    "                cur_rep = np.stack(cur_rep).mean(axis=0)  # HW TODO: change to token level classification\n",
    "                representation.append(cur_rep)\n",
    "                labels.append(cur_labels)\n",
    "            self.labels = labels\n",
    "            representation = np.stack(representation)\n",
    "            self.tokenized_sen = representation\n",
    "            self.vector_dim = representation.shape[-1]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        cur_sen = self.tokenized_sen[item]\n",
    "        if self.vector_type == 'tf-idf':\n",
    "            cur_sen = torch.FloatTensor(cur_sen.toarray()).squeeze()\n",
    "        else:\n",
    "            cur_sen = torch.FloatTensor(cur_sen).squeeze()\n",
    "        label = self.labels[item]\n",
    "        label = self.tags_to_idx[label]\n",
    "        data = {\"input_ids\": cur_sen, \"labels\": label}\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class SentimentNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vec_dim, num_classes, hidden_dim=100):\n",
    "        super(SentimentNN, self).__init__()\n",
    "        self.first_layer = nn.Linear(vec_dim, hidden_dim)\n",
    "        self.second_layer = nn.Linear(hidden_dim, num_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        x = self.first_layer(input_ids)\n",
    "        x = self.activation(x)\n",
    "        x = self.second_layer(x)\n",
    "        if labels is None:\n",
    "            return x, None\n",
    "        loss = self.loss(x, labels)\n",
    "        return x, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def train(model, data_sets, optimizer, num_epochs: int, batch_size=16):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    data_loaders = {\"train\": DataLoader(data_sets[\"train\"], batch_size=batch_size, shuffle=True),\n",
    "                    \"test\": DataLoader(data_sets[\"test\"], batch_size=batch_size, shuffle=False)}\n",
    "    model.to(device)\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'test']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            labels, preds = [], []\n",
    "\n",
    "            for batch in data_loaders[phase]:\n",
    "                batch_size = 0\n",
    "                for k, v in batch.items():\n",
    "                    batch[k] = v.to(device)\n",
    "                    batch_size = v.shape[0]\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                if phase == 'train':\n",
    "                    outputs, loss = model(**batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        outputs, loss = model(**batch)\n",
    "                pred = outputs.argmax(dim=-1).clone().detach().cpu()\n",
    "                labels += batch['labels'].cpu().view(-1).tolist()\n",
    "                preds += pred.view(-1).tolist()\n",
    "                running_loss += loss.item() * batch_size\n",
    "\n",
    "            epoch_loss = running_loss / len(data_sets[phase])\n",
    "            epoch_acc = accuracy_score(labels, preds)\n",
    "\n",
    "            epoch_acc = round(epoch_acc, 5)\n",
    "\n",
    "            if phase.title() == \"test\":\n",
    "                print(f'{phase.title()} Loss: {epoch_loss:.4e} Accuracy: {epoch_acc}')\n",
    "            else:\n",
    "                print(f'{phase.title()} Loss: {epoch_loss:.4e} Accuracy: {epoch_acc}')\n",
    "            if phase == 'test' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                with open('model.pkl', 'wb') as f:\n",
    "                    torch.save(model, f)\n",
    "        print()\n",
    "\n",
    "    print(f'Best Validation Accuracy: {best_acc:4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created train\n",
      "Epoch 1/5\n",
      "----------\n",
      "Train Loss: 3.4617e-01 Accuracy: 0.84957\n",
      "Test Loss: 3.0488e-01 Accuracy: 0.86907\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "Train Loss: 2.0458e-01 Accuracy: 0.92113\n",
      "Test Loss: 3.4951e-01 Accuracy: 0.85601\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n",
      "Train Loss: 1.3632e-01 Accuracy: 0.95003\n",
      "Test Loss: 4.0333e-01 Accuracy: 0.84577\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n",
      "Train Loss: 8.8921e-02 Accuracy: 0.97016\n",
      "Test Loss: 5.2375e-01 Accuracy: 0.84227\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n",
      "Train Loss: 5.4734e-02 Accuracy: 0.98208\n",
      "Test Loss: 6.1827e-01 Accuracy: 0.83796\n",
      "\n",
      "Best Validation Accuracy: 0.869070\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ds = SentimentDataSet('amazon_sa/train.csv', vector_type='tf-idf')\n",
    "print('created train')\n",
    "test_ds = SentimentDataSet('amazon_sa/test.csv', vector_type='tf-idf', tokenizer=train_ds.tokenizer)\n",
    "datasets = {\"train\": train_ds, \"test\": test_ds}\n",
    "nn_model = SentimentNN(num_classes=2, vec_dim=train_ds.vector_dim)\n",
    "optimizer = Adam(params=nn_model.parameters())\n",
    "train(model=nn_model, data_sets=datasets, optimizer=optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word to Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ngtkl;jrgsh'tldfk lsdfmkhgk lknfgh lkjfgnh'lk kdfngh'lkn sfdngylekn lkjtdhlk lkthtk lksdhyklt klfntuhl;rkj lskdfhlk;j lkrftghlk lk'srthlk sfkhlk lkngtrhlk fghklnlk sgkjhbkj lkrlkhk rlkhjlk sdflghkj cannot be represented!\n",
      "Sentence vjnmhrg;h lndsfg;okhi d;jfnhgtoeruihj lkjsdlfio jajghoshi khsdtrhjo'i jlakhdgrkjh klagtrjkl'j lkdsnrtylorhjo ijlektj blkjtrl'ykhje'kj lknsfhytrlkhn ljknsghjytr'lkj lkjstl'rkyjl'kj lklgtkjhytrlj lkjtrylkrjlkj ltkjyrkjd lkftrylkrjl jlkjtrylrkgj fhj cannot be represented!\n",
      "created train\n",
      "Sentence Zzzzzzzzzzz! cannot be represented!\n",
      "Epoch 1/5\n",
      "----------\n",
      "Train Loss: 3.9117e-01 Accuracy: 0.81602\n",
      "Test Loss: 3.6184e-01 Accuracy: 0.83268\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "Train Loss: 3.4927e-01 Accuracy: 0.84188\n",
      "Test Loss: 3.5507e-01 Accuracy: 0.84036\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n",
      "Train Loss: 3.4362e-01 Accuracy: 0.8437\n",
      "Test Loss: 3.5076e-01 Accuracy: 0.84184\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n",
      "Train Loss: 3.4036e-01 Accuracy: 0.84646\n",
      "Test Loss: 3.4979e-01 Accuracy: 0.84373\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n",
      "Train Loss: 3.3800e-01 Accuracy: 0.84821\n",
      "Test Loss: 3.4912e-01 Accuracy: 0.8444\n",
      "\n",
      "Best Validation Accuracy: 0.844400\n"
     ]
    }
   ],
   "source": [
    "train_ds = SentimentDataSet('amazon_sa/train.csv', vector_type='w2v')\n",
    "print('created train')\n",
    "test_ds = SentimentDataSet('amazon_sa/test.csv', vector_type='w2v')\n",
    "datasets = {\"train\": train_ds, \"test\": test_ds}\n",
    "nn_model = SentimentNN(num_classes=2, vec_dim=train_ds.vector_dim)\n",
    "optimizer = Adam(params=nn_model.parameters())\n",
    "train(model=nn_model, data_sets=datasets, optimizer=optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence ngtkl;jrgsh'tldfk lsdfmkhgk lknfgh lkjfgnh'lk kdfngh'lkn sfdngylekn lkjtdhlk lkthtk lksdhyklt klfntuhl;rkj lskdfhlk;j lkrftghlk lk'srthlk sfkhlk lkngtrhlk fghklnlk sgkjhbkj lkrlkhk rlkhjlk sdflghkj cannot be represented!\n",
      "created train\n",
      "Epoch 1/5\n",
      "----------\n",
      "Train Loss: 3.8406e-01 Accuracy: 0.8236\n",
      "Test Loss: 3.7604e-01 Accuracy: 0.82907\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "Train Loss: 3.5778e-01 Accuracy: 0.83647\n",
      "Test Loss: 3.6485e-01 Accuracy: 0.83432\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n",
      "Train Loss: 3.5267e-01 Accuracy: 0.84047\n",
      "Test Loss: 3.6361e-01 Accuracy: 0.83688\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n",
      "Train Loss: 3.4851e-01 Accuracy: 0.84094\n",
      "Test Loss: 3.6156e-01 Accuracy: 0.83621\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n",
      "Train Loss: 3.4593e-01 Accuracy: 0.84347\n",
      "Test Loss: 3.6611e-01 Accuracy: 0.83446\n",
      "\n",
      "Best Validation Accuracy: 0.836880\n"
     ]
    }
   ],
   "source": [
    "train_ds = SentimentDataSet('amazon_sa/train.csv', vector_type='glove')\n",
    "print('created train')\n",
    "test_ds = SentimentDataSet('amazon_sa/test.csv', vector_type='glove')\n",
    "datasets = {\"train\": train_ds, \"test\": test_ds}\n",
    "nn_model = SentimentNN(num_classes=2, vec_dim=train_ds.vector_dim)\n",
    "optimizer = Adam(params=nn_model.parameters())\n",
    "train(model=nn_model, data_sets=datasets, optimizer=optimizer, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial 04 - word embeddings.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
